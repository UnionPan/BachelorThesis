Word Re-ordering and DP-based Search in Statistical Machine Translation
In this paper , we describe a search procedure for statistical machine translation  .
Starting from a DP-based solution to the traveling salesman problem , we present a novel technique to restrict the possible word reordering between source and target language in order to achieve an eÆcient search algorithm .
The experimental tests are carried out on the Verbmobil task  , which is a limited-domain spoken-language task .
The goal of machine translation is the translation of a text given in some source language into a target language .
Our approach uses word-to-word dependencies between source and target words .
The model is often further restricted so that each source word is assigned to exactly one target word  .
These alignment models are similar to the concept of hidden Markov models  in speech recognition .
The alignment mapping is j ! i = aj from source position j to target position i = aj . The use of this alignment model raises major problems if a source word has to be aligned to several target words , e.g . when translating German compound nouns .
A simple extension will be used to handle this problem .
In Section 2 , we brie y review our approach to statistical machine translation .
In Section 3 , we introduce our novel concept to word reordering and a DP-based search , which is especially suitable for the translation direction from German to English .
This approach is compared to another reordering scheme presented in  .
In Section 4 , we present the performance measures used and give translation results on the Verbmobil task .
In this section , we brie y review our translation approach .
In Eq .
 is the language model , which is a trigram language model in this case .
For the translation model Pr  , we go on the assumption that each source word is aligned to exactly one target word .
The alignment model uses two kinds of parameters : alignment probabilities p  .
When aligning the words in parallel texts  , we typically observe a strong localization effect .
In many cases , there is an even stronger restriction : over large portions of the source string , the alignment is monotone .
2.1 Inverted Alignments .
To explicitly handle the word reordering between words in source and target language , we use the concept of the so-called inverted alignments as given in  .
An inverted alignment is defined as follows : inverted alignment : i ! j = bi : Target positions i are mapped to source positions bi .
What is important and is not expressed by the notation is the so-called coverage constraint : each source position j should be 'hit ' exactly once by the path of the inverted alignment bI 1 = b1 : : : bi : : : bI . Using the inverted alignments in the maximum approximation , we obtain as search criterion : max I  is the trigram language model probability .
The inverted alignment probability p  are obtained by relative frequency estimates from the Viterbi alignment path after the final training iteration .
The details are given in  .
The sentence length probability p  is omitted without any loss in performance .
The baseline alignment model does not permit that a source word is aligned to two or more target words , e.g . for the translation direction from German toEnglish , the German compound noun 'Zahnarztter min ' causes problems , because it must be translated by the two target words dentist 's appointment .
We use a solution to this problem similar to the one presented in  , where target words are joined during training .
The word joining is done on the basis of a likelihood criterion .
An extended lexicon model is defined , and its likelihood is compared to a baseline lexicon model , which takes only single-word dependencies into account .
In the following , we assume that this word joining has been carried out .
In order to handle the necessary word reordering as an optimization problem within our dynamic programming approach , we describe a solution to the traveling salesman problem  .
The traveling salesman problem is an optimization problem which is defined as follows : given are a set of cities S = s1 ; ; sn and for each pair of cities si ; sj the cost dij > 0 for traveling from city si to city sj . We are looking for the shortest tour visiting all cities exactly once while starting and ending in city s1 .
A straightforward way to find the shortest tour is by trying all possible permutations of the n cities .
The resulting algorithm has a complexity of O  .
However , dynamic programming can be used to find the shortest tour in exponential time , namely in O  , using the algorithm by Held and Karp .
The approach recursively evaluates a quantity Q  , where C is the set of already visited cities and sj is the last visited city .
Subsets C of increasing cardinality c are processed .
The algorithm works due to the fact that not all permutations of cities have to be considered explicitly .
For a given partial hypothesis  , only the score for the best path reaching j has to be stored .
This algorithm can be applied to statistical machine translation .
Using the concept of inverted alignments , we explicitly take care of the coverage constraint by introducing a coverage set C of source sentence positions that have been already processed .
The advantage is that we can recombine search hypotheses by dynamic programming .
The cities of the traveling salesman problem correspond to source Table 1 : DP algorithm for statistical machine translation .
Subsets of partial hypotheses with coverage sets C of increasing cardinality c are processed .
For a trigram language model , the partial hypotheses are of the form  .
e0 ; e are the last two target words , C is a coverage set for the already covered source positions and j is the last position visited .
Each distance in the traveling salesman problem now corresponds to the negative logarithm of the product of the translation , alignment and language model probabilities .
The type of alignment we have considered so far requires the same length for source and target sentence , i.e . I = J. Evidently , this is an unrealistic assumption , therefore we extend the concept of inverted alignments as follows : When adding a new position to the coverage set C , we might generate either Æ = 0 or Æ = 1 new target words .
For Æ = 1 , a new target language word is generated using the trigram language model p  .
For Æ = 0 , no new target word is generated , while an additional source sentence position is covered .
The above auxiliary quantity satisfies the following recursive DP equation : Qe0  = Initial Skip Verb Final 1 .
In .
Fall .
Figure 2 : Order in which source positions are visited for the example given in Fig.1 .
The resulting algorithm is depicted in Table 1 .
Restrictions : Quasi-monotone Search The above search space is still too large to allow the translation of a medium length input sentence .
On the other hand , only very restricted reorderings are necessary , e.g . for the translation direction from Table 2 : Coverage set hypothesis extensions for the IBM reordering .
No : Predecessor coverage set Successor coverage set 1  !
In German , the verbgroup usually consists of a left and a right verbal brace , whereas in English the words of the verbgroup usually form a sequence of consecutive words .
Our new approach , which is called quasi-monotone search , processes the source sentence monotonically , while explicitly taking into account the positions of the German verbgroup .
A typical situation is shown in Figure 1 .
The translation of one position in the source sentence may be postponed for up to L = 3 source positions , and the translation of up to two source positions may be anticipated for at most R = 10 source positions .
To formalize the approach , we introduce four verbgroup states S : Initial  : A contiguous , initial block of source positions is covered .
Skipped  : The translation of up to two words may be anticipated .
Final  : The rest of the sentence is processed monotonically taking account of the already covered positions .
While processing the source sentence monotonically , the initial state I is entered whenever there are no uncovered positions to the left of the rightmost covered position .
The sequence of states needed to carry out the word reordering example in Fig .
1 is given in Fig .
The 13 positions of the source sentence are processed in the order shown .
A position is presented by the word at that position .
There are 13 types of extensions needed to describe the verbgroup reordering .
The details are given in  .
For each extension a new position is added to the coverage set .
Covering the first uncovered position in the source sentence , we use the language model probability p  .
Here , $ is the sentence boundary symbol , which is thought to be at position 0 in the target sentence .
The search starts in the hypothesis  .
f ; g denotes the empty set , where no source sentence position is covered .
The following recursive equation is evaluated : Qe0  .
The proof is given in  .
Restrictions We compare our new approach with the word reordering used in the IBM translation approach  .
A detailed description of the search procedure used is given in this patent .
Source sentence words are aligned with hypothesized target sentence words , where the choice of a new source word , which has not been aligned with a target word yet , is restricted1 .
A procedural definition to restrict1In the approach described in  , a mor phological analysis is carried out and word morphemes rather than full-form words are used during the search .
Here , we process only full-form words within the translation procedure .
the number of permutations carried out for the word reordering is given .
During the search process , a partial hypothesis is extended by choosing a source sentence position , which has not been aligned with a target sentence position yet .
Only one of the first n positions which are not already aligned in a partial hypothesis may be chosen , where n is set to 4 .
The restriction can be expressed in terms of the number of uncovered source sentence positions to the left of the rightmost position m in the coverage set .
Otherwise for the predecessor search hypothesis , we would have chosen a position that would not have been among the first n uncovered positions .
Ignoring the identity of the target language words e and e0 , the possible partial hypothesis extensions due to the IBM restrictions are shown in Table 2 .
Note that in line 4 the last visited position for the successor hypothesis must be m. Otherwise , there will be four uncovered positions for the predecessor hypothesis violating the restriction .
A dynamic programming recursion similar to the one in Eq . 2 is evaluated .
In this case , we have no finite-state restrictions for the search space .
This approach leads to a search procedure with complexity O  .
The proof is given in  .
We have tested the translation system on the Verbmobil task  .
The Verbmobil task is an appointment scheduling task .
Two subjects are each given a calendar and they are asked to schedule a meeting .
The translation direction is from German to English .
A summary of the corpus used in the experiments is given in Table 3 .
The perplexity for the trigram language model used is 26:5 .
Although the ultimate goal of the Verbmobil project is the translation of spoken language , the input used for the translation experiments reported on in this paper is the  correct orthographic transcription of the spoken sentences .
Thus , the effects of spontaneous speech are present in the corpus , e.g . the syntactic structure of the sentence is rather less restricted , however the effect of speech recognition errors is not covered .
German city names are replaced by category markers .
The translation search is carried out with the category markers and the city names are resubstituted into the target sentence as a postprocessing step .
The following two error criteria are used in our experiments : mWER : multi-reference WER : We use the Levenshtein distance between the automatic translation and several reference translations as a measure of the translation errors .
On average , 6 reference translations per automatic translation are available .
The Levenshtein distance between the automatic translation and each of the reference translations is computed , and the minimum Levenshtein distance is taken .
This measure has the advantage of being completely automatic .
SSER : subjective sentence error rate : For a more detailed analysis , the translations are judged by a human test person .
An error count of 0:0 is assigned to a perfect translation , and an error count of 1:0 is assigned to a semantically and syntactically wrong translation .
4.3 Translation Experiments .
We apply a beam search concept as in speech recognition .
However there is no global pruning .
Search hypotheses are processed separately according to their coverage set C. The best scored hypothesis for each coverage set is computed : QBeam  ; where t0 is a threshold to control the number of surviving hypotheses .
Additionally , for a given coverage set , at most 250 different hypotheses are kept during the search process , and the number of different words to be hypothesized by a source word is limited .
For each source word f , the list of its possible translations e is sorted according to p  is the unigram probability of the English word e. It is suÆcient to consider only the best 50 words .
We show translation results for three approaches : the monotone search  search as described in Section 3.2 .
Table 4 shows translation results for the three approaches .
The computing time is given in terms of CPU time per sentence  .
Translation errors are reported in terms of multireference word error rate  .
The monotone search performs worst in terms of both error rates mWER and SSER .
The computing time is low , since no reordering is carried out .
The quasi-monotone search performs best in terms of both error rates mWER and SSER .
Additionally , it works about 3 times as fast as the IBM style search .
For our demonstration system , we typically use the pruning threshold t0 = 5:0 to speed up the search by a factor 5 while allowing for a small degradation in translation accuracy .
The effect of the pruning threshold t0 is shown in Table 5 .
The negative logarithm of t0 is reported .
The translation scores for the hypotheses generated with different threshold values t0 are compared to the translation scores obtained with a conservatively large threshold t0 = 10:0 . For each test series , we count the number of sentences whose score is worse than the corresponding score of the test series with the conservatively large threshold t0 = 10:0 , and this number is reported as the number of search errors .
Depending on the threshold t0 , the search algorithm may miss the globally optimal path which typically results in additional translation errors .
Decreasing the threshold results in higher mWER due to additional search errors .
Table 5 : Effect of the beam threshold on the number of search errors  .
Again , the monotone search performs worst .
In the second and third translation examples , the IbmS word reordering performs worse than the QmS word reordering , since it can not take properly into account the word reordering due to the German verbgroup .
In the last example , the less restrictive IbmS word reordering leads to a better translation , although the QmS translation is still acceptable .
In this paper , we have presented a new , eÆcient DP-based search procedure for statistical machine translation .
The approach assumes that the word reordering is restricted to a few positions in the source sentence .
The approach has been successfully tested on the 8 000-word Verbmobil task .
Future extensions of the system might include : 1 ) An extended translation model , where we use more context to predict a source word .
2 ) An improved language model , which takes into account syntactic structure , e.g . to ensure that a proper English verbgroup is generated .
This work has been supported as part of the Verbmobil project  by the European Community .
